# L3xbot ğŸš€  

## **Vision**  
L3xbot AI-Powered Vision Assistance for People with Diverse Visual Needs**

L3xbot is designed to empower individuals with **low vision, blindness, or diverse visual needs** by transforming **perception into accessibility** through **computer vision, AI guidance, and haptic feedback**. Created by **Naomi McQuade**, L3xbot is built with a deep understanding of how technology can enhance sensory experience and enable new ways of interacting with the world.

## **Features**  
- ğŸ· **Object Recognition** â€“ Uses YOLO-based detection to identify objects in real-time.  
- ğŸ”Š **Speech Feedback** â€“ Converts detected objects into spoken descriptions using text-to-speech.  
- ğŸ“³ **Haptic Alerts** â€“ Sends vibration signals when obstacles are detected to assist navigation.  
- ğŸŒ **Accessible Design** â€“ Built with an intuitive user interface for seamless interaction.  

## **Tech Stack**  
- ğŸ”¹ **Python** â€“ Core language for AI processing  
- ğŸ”¹ **OpenCV** â€“ Image recognition and computer vision  
- ğŸ”¹ **YOLOv3** â€“ Deep learning model for object detection  
- ğŸ”¹ **Google Text-to-Speech (gTTS)** â€“ Voice feedback  
- ğŸ”¹ **Arduino/Raspberry Pi** â€“ Hardware integration for haptic feedback  

## **Getting Started**  
### **1. Clone the Repository**  
```bash
git clone https://github.com/YOUR-USERNAME/L3xbot.git
cd L3xbot
