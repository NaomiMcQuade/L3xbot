# L3xbot 🚀  

## **Vision**  
L3xbot AI-Powered Vision Assistance for People with Diverse Visual Needs**

L3xbot is designed to empower individuals with **low vision, blindness, or diverse visual needs** by transforming **perception into accessibility** through **computer vision, AI guidance, and haptic feedback**. Created by **Naomi McQuade**, L3xbot is built with a deep understanding of how technology can enhance sensory experience and enable new ways of interacting with the world.

## **Features**  
- 🏷 **Object Recognition** – Uses YOLO-based detection to identify objects in real-time.  
- 🔊 **Speech Feedback** – Converts detected objects into spoken descriptions using text-to-speech.  
- 📳 **Haptic Alerts** – Sends vibration signals when obstacles are detected to assist navigation.  
- 🌍 **Accessible Design** – Built with an intuitive user interface for seamless interaction.  

## **Tech Stack**  
- 🔹 **Python** – Core language for AI processing  
- 🔹 **OpenCV** – Image recognition and computer vision  
- 🔹 **YOLOv3** – Deep learning model for object detection  
- 🔹 **Google Text-to-Speech (gTTS)** – Voice feedback  
- 🔹 **Arduino/Raspberry Pi** – Hardware integration for haptic feedback  

## **Getting Started**  
### **1. Clone the Repository**  
```bash
git clone https://github.com/YOUR-USERNAME/L3xbot.git
cd L3xbot
